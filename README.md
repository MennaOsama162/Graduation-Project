# Our work involves the development of a vision-based system that translates certain gestures in Egyptian Sign Language into their corresponding isolated words and Arabic sign language alphabetic letters. To achieve this, we utilized MediaPipe for both live and streaming media and created an automated database to store our data. Two different architectures were employed for classification purposes:	The first architecture, LSTM, was used for Egyptian Sign Language words, resulting in an accuracy of 92.7%. 
The second architecture, CNN, was used for Arabic sign language alphabetic letters, resulting in an accuracy of 90%.
